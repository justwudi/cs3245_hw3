1) To support phrasal queries, each document in the postings list will have to 
store the positions of where the terms appear in that document.

Document: to be or not to be
Tokenize and stem: ['to', 'be', 'or', 'not', 'to', 'be']

Dictionary:
{
    'to':  [(document, [0,4])],
    'be':  [(document, [1,5])],
    'or':  [(document, 2)],
    'not': [(document, 3)]
}

If we are querying for 'not to be', we will intersect the posting lists based 
on the same document, and positions 1 from each other. The document will then 
be appended to the intermediate result, together with the array of ending 
positions that match the phrase (meaning that the positions are 1 from each 
other).

Intersect 'not' with 'to', when we have the same document in the posting list,
we will match the position 3 for 'not' and position 4 for 'to', thus the 
intermediate result will contain 4 and discard 0 to give [(document, [4])].

Following this logic, the final result will contain (document, [5]) and thus we
will return document.

However if we are searching for 'or to be', the search will end at the 
intersection between 'or' and 'to' since there are no continuous positions for 
both words.


2) Longer documents will have more diversity, thus will contain more terms and 
have higher tf values. These will raise the scores of longer documents as 
compared to a shorter document. For longer queries, due to the increase in the 
number of query items, a larger set of results will be returned mainly because 
we are doing union of the postings list for each query item. However if we are 
just comparing between the results of long document and long queries, since all 
documents are long, the tf is relative and will still produce relevant results. 
As for the longer queries, documents will have to match a higher number of 
query items in order to obtain a higher score. Thus the results obtained will 
be as relevant as compared to short document and short queries.

The normalization process that we are using will not be sufficient to address 
for the problem of the different in length of documents. The normalization 
process elminiated the information on the length of the origianl document, and 
thus we will treat long and short document on equal grounds despite the fact 
that longer document will produce a heigher weight. However, following the 
dicussion before this, if we are always comparing between long document or 
short document, the effect will not be as obvious.

For short document with short query, each term will contribute a larger score, 
and it is less likely for a irrelavant document to hit upon a search query. 
Thus, the result returns will be much more relavant.

Assumption: ltc.lnc calculate use N(number of queries) and df(number of query which contain the term) (Since the alternative will not change anything)

In my judgement, the ltc.lnc scheme should be sufficient to retrieve the
result from the reuter sets. Since the set of queries is probably not large,
there will not be much benefit from the idf calculate from the queries.
However, it should be enough to still get relavant result despite not
calculating idf since the term in reuters set are relatively distinct and
unique. 

3) Field parametric depends on the consistency of the fields among the 
documents in order to be useful. Given that Reuters collection do not have a 
uniform quality for its meta data, it is unlikely that they have consistent 
field parametric for each document. Thus, parametric indices will not be useful 
for practical search in the Reuters collection.

Since each Reuter documents actually contains a minimun of titles and its 
content zone. Making use of this two information will certainly be able to 
provide more context for the search engine than those which is not using zone 
at all. Thus, zone would be useful for practical search in Reuters collection.